% The master copy of this demo dissertation is held on my filespace
% on the cl file serve (/homes/mr/teaching/demodissert/)

% Last updated by MR on 2 August 2001

\documentclass[12pt,twoside,notitlepage]{report}

\usepackage{a4}
\usepackage{verbatim}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{minted}
%\usemintedstyle{colorful}
\setmintedinline{breaklines}

\newcommand{\textinline}{\mintinline{text}}
\newcommand{\cinline}{\mintinline{C}}
\newcommand{\camlinline}{\mintinline{OCaml}}

\newcommand\note[1]{\textcolor{blue}{#1}}

\input{epsf}                            % to allow postscript inclusions
% On thor and CUS read top of file:
%     /opt/TeX/lib/texmf/tex/dvips/epsf.sty
% On CL machines read:
%     /usr/lib/tex/macros/dvips/epsf.tex



\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\usepackage[backend=bibtex, style=alphabetic, sorting=ynt]{biblatex}
\addbibresource{refs.bib}

\begin{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\hfill{\LARGE \bf Paul Durbaba}

\vspace*{60mm}
\begin{center}
\Huge
{\bf Compiling OCaml to WebAssembly} \\
\vspace*{5mm}
Diploma in Computer Science \\
\vspace*{5mm}
Robinson College \\
\vspace*{5mm}
May 2020  % today's date
\end{center}

\clearpage


 
\newpage
\section*{Declaration}

I, Paul Durbaba of Robinson College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed Paul Durbaba}

\medskip
\leftline{Date [date]}

\section*{Acknowledgements}

% TODO List the people that check the diss
\note{LIST THE PEOPLE THAT CHECK THE DISS}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
	\begin{tabular}{ll}
		Name:               & \bf Paul Durbaba                       \\
		College:            & \bf Robinson College                     \\
		Project Title:      & \bf Compiling OCaml to WebAssembly \\
		Examination:        & \bf Part II Computer Science, May 2020        \\
		Word Count:         & \bf FILL IN LATER  \\
		Project Originator: & Timothy M. Jones                \\
		Supervisor:         & Tobias Kohn            \\ 
	\end{tabular}
}
%\footnotetext[1]{This word count was computed
%	by {\tt detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
%}
%\stepcounter{footnote}


\section*{Original Aims of the Project}

\note{At most 100 words describing the original aims of the project.}


\section*{Work Completed}

\note{At most 100 words summarising the work completed.}

\section*{Special Difficulties}

\note{At most 100 words describing any special difficulties that you faced.
(In most cases the special difficulties entry will say “None”.) }

\tableofcontents

\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\clearpage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}

\chapter{Introduction}
\note{The Introduction should explain the principal motivation for the project. Show how the work fits into the broad area of surrounding Computer Science and give a brief survey of previous related work. It should generally be unnecessary to quote at length from technical papers or textbooks. If a simple bibliographic reference is insufficient, consign any lengthy quotation to an appendix.}
%TODO EXPLAIN KEY MOTIVATION, IDEAS BEHIND PROJECT
My motivation for this project is to learn how to make a compiler. Compilers are essential to computing because they enable the transformation of code from high-level languages that are intuitive for use by us, into low-level machine understandable code than can actually be executed, and writing a compiler is a substantial software project that invokes many areas of computer science such as type theory and program analysis.

% TODO PREVIOUS RELATED WORK

\section{OCaml}
OCaml\cite{OCaml} is a strongly-typed functional programming language, with some imperative features such as references. I chose OCaml both as the source language of the compiler, and the language the compiler is designed in, because I wanted to gain some familiarity in writing programs in functional languages, and OCaml has similar syntax to Standard ML taught in first year, but with much better library support, and because compiling a functional programming language presents additional challenges to compiling an imperative language such as C - with first class functions and pattern matching requiring special consideration.

\section{WebAssembly}
% copied from project proposal
WebAssembly\cite{webassembly} is a stack-based binary instruction format for the web with the main goal of improving performance  of  more  computationally  intensive  functions  in  web  applications.  It  does not  replace  JavaScript  as  there  (currently)  is  no  way  to  perform  tasks  such  as  DOM manipulation  directly  from  WebAssembly  ---  it  is  expected  that  a  JavaScript  application might  call  some  functions  implemented  in  WebAssembly  to  perform  computation,  and then display the results itself.
\\\\
The current MVP (minimum viable product) version of WebAssembly is designed for compiling languages like C and C++ that do not use garbage collection and can make do without exceptions. There are extensions currently being developed to add support for these features, but progress on these is slow as they often depend on other extensions, for instance the garbage collection extension depends on extensions for reference types and typed function references, which seek to expand the WebAssembly type system so for instance a garbage collector would understand the shape of data in memory.\cite{Wgce}
\\\\
WebAssembly was chosen as the target instruction set because it is relatively new, with few compilers out there currently targeting it, and it is likely to grow in popularity in the future as more extensions are added to it that make it more viable to be used - such as support for garbage collection and exceptions.
% TODO

%TODO WHAT IS WEBASSEMBLY


% TODO DESCRIPTION OF HOW TO BUILD THE PROJECT?

\section{Related Work}
% TODO Js\_of\_ocaml, A

There have been a few attempts to compile OCaml to WebAssembly already, such as by @SanderSpies\cite{Awbfo}, who modified the existing backend of the OCaml Compiler to target WebAssembly. Their attempt worked from the `CMM' --- the final stage of the OCaml compiler before code generation, modifying that to include extra type information for WebAssembly, and then doing code generation. This differs from my approach in that I am implementing almost an entire compiler from the type-checker through to the WebAssembly code generator, but excluding lexing/parsing.
\\\\
While Sander's approach allows them to leverage the existing features and optimisations of the OCaml compiler, their approach didn't fit with my goals of learning how to write an entire compiler - including type checker, intermediate translations and optimisations by myself, and such an approach likely wouldn't constitute enough work for a Part II project.


\clearpage



\chapter{Preparation}
\note{Principally, this chapter should describe the work which was undertaken before code was written, hardware built or theories worked on. It should show how the project proposal was further refined and clarified, so that the Implementation stage could go smoothly rather than by trial and error.}

\note{Throughout this chapter and indeed the whole dissertation, it is essential to demonstrate that a proper professional approach was employed.}

\note{The nature of this chapter will vary greatly from one dissertation to another but, underlining the professional approach, this chapter will very likely include a section headed “Requirements Analysis” and incorporate other references to software engineering techniques.}

\note{The chapter will cite any new programming languages and systems which had to be learnt and will mention complicated theories or algorithms which required understanding.}

\note{It is essential to declare the Starting Point (see Section 7). This states any existing codebase or materials that your project builds on. The text here can commonly be identical to the text in your proposal, but it may enlarge on it or report variations. For instance, the true starting point may have turned out to be different from that declared in the proposal and such discrepancies must be explained. }

\section{Requirements}

The success criteria in the project proposal presents a clearly defined subset of OCaml to implement. This subset was designed to be large enough so that useful OCaml programs could be written in it, while small enough to be feasible to implement by Christmas.

% TODO STARTING POINT

% TODO MATERIAL DONE BEFORE CODE WAS WRITTEN

% TODO? HOW I ENSURED CODING WASN'T TRIAL AND ERROR

\section{Components of the Compiler}


\section{Working Environment and Tools Setup}
Since I have both a laptop and desktop, I decided the best way to work on both would be to do the work remotely via a remote desktop application on a remote server. Both my laptop and desktop were configured to download backups from the server once per hour (if they were on), so if there proved to be a problem with the server, I could redeploy easily by uploading the backups to a new server if required.
\\\\
In addition, I used Git in order to keep a record of my work, to allow me to access previous versions of files, and to backup to GitHub

\subsection{Dune}
I chose to use the Dune\cite{Dune} build system for OCaml as it is the most widely-used build system for OCaml, and supports multi-module projects and dependencies installed via OPAM, the OCaml Package Manager.
\\\\
Dune build files are specified in each directory a file called \textinline{dune}, with the top-level directory specifying the build file for the entire project, and subdirectories containing the build files for each module. These build files are specified in a LISP-like `s-expression' syntax, for instance here is the top-level build file of the project:
\\\\
\begin{minipage}{\linewidth}

\begin{minted}{LISP}
(executable
    (name toplevel)
    (libraries proj.types proj.transform proj.codegen proj.base
               core_kernel compiler-libs.common)
    (preprocess (pps ppx_jane)))
\end{minted}
\end{minipage}
\\\\
The name field includes the `public names' of libraries to be included. Library modules have their own build files that specify `library' instead of `executable', and an additional `public\_name' field.
\\\\
The executable can be build by invoking \textinline{dune build toplevel.exe} in the top-level directory, which will output the binary to \textinline{_build/default/toplevel.exe}.

% TODO Dune, Git

\section{Libraries / Tools Used}
\subsection{OCaml Compiler Libs}
I used the official OCaml compiler libraries to perform lexing and parsing, which provide the same frontend as used by the official OCaml compiler. As the frontend of the official OCaml compiler is liable to change between releases, I stuck to version 4.08, which was the most recent version when I started the project but has now been succeeded by 4.09.
\note{Citation needed}

\subsection{Web Assembly Binary Toolkit}
The Web Assembly Binary Toolkit is a separate tool which can compile WebAssembly Text Form (.wast files) outputted by my compiler to the WebAssembly Binary Format (.wasm files) that can be loaded by NodeJS/browsers.
\note{Citation needed}

\section{Starting Point}
I had very little OCaml experience prior to this project, and no recent experience of writing compilers besides from writing a mathematical expression interpreter a few years ago. In addition, I had no experience of using WebAssembly, but I have plenty of experience writing JavaScript.
\\\\
I attempted to deal with some of these issues prior to starting the project by coming up with some OCaml samples for the compiler to compile in the future, and by setting up an OCaml workspace where I successfully figured out how to import the OCaml Compiler libraries, and learned to navigate the AST they use by writing a simple example that adds one to integer constants. I also read through the WebAssembly documentation to get a sense of which features would be a challenge to compile to WebAssembly.

\clearpage
\chapter{Implementation}
\note{This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage might profitably be referred to (the professional approach again).}

\note{Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left to appendices or omitted altogether. Analogous advice applies to circuit diagrams.}

\note{Draw attention to the parts of the work which are not your own. The Implementation Chapter should include a section labelled "Repository Overview". The repository overview should be around one page in length and should describe the high-level structure of the source code found in your source code Repository. It should describe whether the code was written from scratch or if it built on an existing project or tutorial. Making effective use of powerful tools and pre-existing code is often laudable, and will count to your credit if properly reported.}

\note{It should not be necessary to give a day-by-day account of the progress of the work but major milestones may sometimes be highlighted with advantage. }

% TODO WORK DONE, ONE SECTION PER PART OF THE COMPILER



\section{Front End}
Lexing and parsing is handled by the OCaml Compiler Libs, which produces a \textinline{structure} object representing the AST of the entire program. This AST and the corresponding typed-AST, which I designed to be very similar (using the same names prefixed with a t), are used throughout the first half of the compilation process before intermediate translation occurs, and as such, I will give an overview of their components:
\begin{itemize}
	\item \textinline{(t)structure} A list of \textinline{(t)structure_item} representing the top-level items in a file.
	\item \textinline{(t)structure_item} A single top-level item in a file. Can either be an \textinline{(t)expression} (e.g. for imperative code), a let binding consisting of a list of \textinline{(t)value_binding}, or a type definition consisting of a list of \textinline{type_decls}
	\item \textinline{(t)expression} A unit of code that will produce a value. There are various types including identifiers, constants, match statements, function definitions and applications.
	\item \textinline{(t)pattern} A structure of a value, that can be used both to test if a value conforms to this structure (e.g. in a match statement), and to destructure a value into separate variables (e.g. in a let binding). Can be an identifer, constant, tuple of several patterns, or a construct (effectively a named tuple).
	\item \textinline{(t)value_binding} An expression and a pattern. The result of evaluating the expression is destructured and bound to the variables in the pattern, for instance in \camlinline{let x,y = (5 + 2, 3)} the pattern is \camlinline{(x, y)} and \camlinline{(5 + 2, 3)} is the expression
\end{itemize}
\note{Put the typed AST and the IR in an appendix?}





\section{Type Checker}
\note{
	Does multiple things
	\begin{itemize}
		\item HM type inference, generating constraints and then unifying them
		\item Builds up information about type definitions e.g. construct types and their constructors
		\item Makes the typed-ast
		\begin{itemize}
			\item Small differences to make the typed-ast nicer to work with
			\item Variables get additional unique ID
			\item Patterns get a list of variables they define and their types
			\item Value bindings get a similar list with the generalized types
		\end{itemize}
	\end{itemize}
}
The type-checker is responsible for translating the untyped AST into a typed-AST, and in doing so ensuring that the program is well typed, inferring types that are unknown. The main algorithm that I use is a inference algorithm based on the Hindley-Milner type system. This is a constraint based algorithm that builds up a set of constraints about types at different points in the program, and solves them to produce a unification mapping type-variables to types. My implementation was guided by an article about using Hindley-Milner in Haskell\cite{Hmi}.
\\\\
The definitions of the types I used are shown in Figure \ref{fig:types}. A \camlinline{scheme} is a special kind of type which I explain with let-bindings. Type constraints are implemented as \camlinline{scheme_type} pairs.
\begin{figure}
	\begin{minted}[linenos]{OCaml}
	type tvalue = V_unit | V_int | V_bool | V_float
	
	type scheme_type =
	| T_var of string (* Type variable *)
	| T_val of tvalue
	| T_tuple of scheme_type list
	| T_constr of string * scheme_type list (* Constructs *)
	| T_func of scheme_type * scheme_type (* Function type *)
	
	type scheme = Forall of String.Set.t * scheme_type
	\end{minted}
	\caption{The definition of types}
	\label{fig:types}
\end{figure}
\\\\
The main bulk of the algorithm is generating constraints for expressions and patterns. This is implemented as a depth-first traversal of the AST, generating types and constraints for the `leaves' of the tree and then using rules to generate these constraints for internal nodes in the tree. Some of these rules are shown in figure \ref{fig:typerules}, which uses type rules of the form $\Gamma \vdash e : t, C$, where $\Gamma$ is the type-context (stores the mapping of variables to types), e is the expression or pattern being typed, t is the type, and C is the list of constraints (with $@$ used to append lists). $ftv(\Gamma)$ is the set of free type variables in $\Gamma$.
\begin{figure}
	$$\begin{array}{cc}
	\dfrac{x : f \in \Gamma \quad t = \text{instantiate(f)}}{\Gamma \vdash x : t, \{\}} \textsc{IdentExpr} &
	\dfrac{a \notin ftv(\Gamma)}{\Gamma, x : a \vdash x : a, \{\}} \textsc{IdentPat}
	\end{array}$$
	$$\dfrac{\Gamma,\Gamma_1 \vdash p : t_1,\ C_1 \quad \Gamma, \Gamma_1 \vdash e : t_2,\ C_2}{\Gamma \vdash (\text{fun}\ p \rightarrow e) : t_1 \rightarrow t_2,\ C_1\ @\ C_2} \textsc{Fun}$$
	$$\dfrac{\Gamma \vdash x : t_1,\ C_1 \qquad \Gamma \vdash y : t_2,\ C_2 \quad a \notin ftv(\Gamma)}{\Gamma \vdash x\ y : a,\ C_1\ @\  C_2 \ @\  [t_1 = t_2 \rightarrow a]} \textsc{Apply}$$
	$$\dfrac{n \in \mathbb{N}}{\Gamma \vdash n : \text{int}, \{\}} \textsc{ConstantInt}$$
	$$\Gamma, \Gamma_1 \vdash p : t_1, C_1 \quad \Gamma \vdash e_1 : t_2, C_2 \quad C_3 = C_1\ @\ C_2\ @\ [t_1 = t_2] \quad s = \text{solve}(C_3)$$
	$$\text{For each variable added by p, we now substitute s into its type and generalize it over }$$
	$$ \Gamma \text{ to produce } \Gamma_2, \text{ which is } \Gamma_1 \text{ with the generalizations}$$
	$$\text{We extract outer constraints from } C_3 \text{ to produce } C_4$$
	$$\dfrac{\Gamma, \Gamma_2 \vdash e_2 : t_3, C_5}{\Gamma \vdash \text{let}\ p = e_1\ \text{in}\ e_2 : t_3,\ C_4\ @\ C_5} \textsc{Let}$$
	\caption{Some rules of my Hindley-Milner type system\note{Need to explain, also they are very complicated even before I introduce instantiation and generalization with let bindings. Also, inference for patterns takes an input context and produces an output context. A pattern adds a variable to a context, while a let binding would make that forall over it's free types (generalization). Best to make a multi-line let rule with an explanation of each part? And it's possible to have type variables that are not in ftv(Gamma) that are in use}}
	\label{fig:typerules}
\end{figure}





\section{Lambda Lifting / Closure Conversion}
%\note{Replace function definitions with mk\_closure, extracting function to it's own object. Track the free variables inside the function}

Lambda Lifting and Closure Conversion is the process whereby function definitions are extracted from the AST, replacing the original definition site with a special operation that constructs a closure of the original function, passing in the required environment variables as needed.
\begin{figure}[h]
	\begin{minted}[linenos]{OCaml}
(* Curried function definition *)
let sum (x, y) (z, w) = x + y + z + w

(* Curried function representation in AST *)
let sum = (fun (x, y) -> (fun (z, w) -> x + y + z + w))

(* Modified AST and extracted functions *)
let sum = mk_closure $$f_sum ()
$$f_sum (): fun arg_sum -> mk_closure $$f_sum-app (arg_sum)
$$f_sum-app (arg_sum): fun arg_sum-app ->
let (x, y) = arg_sum in
let (z, w) = arg_sum-app in
x + y + z + w
	\end{minted}
	\caption{Curried functions in closure conversion}
	\label{fig:curried}
\end{figure}
\\\\
This is achieved by walking the typed-AST. Each time a function definition is encountered, we must construct an extracted function definition, and modify the AST to include a make-closure operation. This is done in the following order, with figure \ref{fig:curried} used to illustrate:
\begin{enumerate}
	\item We give a unique name the function. There are three possible cases here:
	\begin{itemize}
		\item The function is defined in a let binding (e.g. \camlinline{sum}), in which case we use the name of the variable in the let binding.
		\item The function is defined as the expression of another function. This occurs in curried functions which are represented in the AST as nested function definitions. In this case we take the name of the parent function, and append `-app'. (e.g. \camlinline{sum-app})
		\item The function is defined anonymously, in which case we give it an anonymous name.
	\end{itemize}
	\item For curried functions, we create a new argument (single variable pattern) for each function (e.g. \camlinline{arg-sum}), and put let expressions to bind these arguments to the original patterns inside the body of the innermost function in the curried definition. This ensures that we only need to store one environment variable per curried argument inside the closures of deeper functions. For instance, \camlinline{arg-sum} is stored inside the closure for \camlinline{$$f_sum-app}, instead of needing to store both \camlinline{x} and \camlinline{y}.
	
	\item We recursively apply closure conversion to the expression of the function, to give us the expression to use in the extracted function definition.
	\item We perform a depth-first search of this expression to determine the free variables, which will become `closure arguments' for the extracted function. For instance, \camlinline{arg-sum} is free inside the body of \camlinline{$$f_sum-app}, so it must become a closure argument for this extracted function.
	\item We replace the function definition with a `special' make closure operation, which takes the name of the extracted function and the closure arguments as parameters. \camlinline{mkclosure $$f_sum-app (arg_sum)}
	\item We create an object to represent the extracted function, which contains it's name, argument pattern, expression, and list of closure variables and their types.
\end{enumerate}
The modified top-level typed-AST, along with these extracted function definitions, are then used as the representation of the program until translation into the intermediate representation occurs.





\section{Optimisations on Closure Converted Typed AST}
I then perform two optimisations on this closure-converted typed-AST:
\begin{itemize}
	\item Direct Call Generation replaces applications of closures with direct calls of the corresponding function, if we can be sure the closure is always one of that function. This is the most effective optimisation I implement because it eliminates the overhead of closure calls, which involves wrapper functions to load the closure arguments from the closure before calling the actual function, and it can be applied to curried functions, enabling multiple arguments to be passed at once without constructing the intermediate closures.
	\item Tail Call Optimisation. A function is tail-recursive if it is recursive (calls itself), but only calls itself as the very last operation to return a result, and never performs additional computation after calling itself. Tail-recursive functions can be modified to a loop instead of calling themselves, which avoids the function call overhead of putting an additional frame on the stack.
\end{itemize}
Both of these are implemented through rule-based analysis of abstract values.

\subsection{Direct Call Generation}
Direct Call Generation walks the AST, diving into functions at the location of their make closure operation, determining which expressions and variables at different point are certainly closures for a specific function, and which variables are available at each point. When a closure application is encountered, we do the following:
\begin{enumerate}
	\item Check if the value being applied is known to be a closure to a specific function.
	\item Check if that function's closure variables are available. Because we do not rename closure variables inside functions, it suffices that if a variable e.g. \textinline{x#3} is available outside the closure, and is also a closure variable, then they refer to the same value.
	\item If we have a single application, we can replace directly with a call that provides the necessary closure variables.
	\item If we have a curried application, we must first determine the order to pass the arguments, and then we can pass all the arguments at once along with the closure variables.
\end{enumerate}

\subsection{Tail Call Optimisation}
Before we can apply tail call optimisation to a function, we must first check if it is tail-recursive. To do this we use a rule-based analysis with three possible `types': simple, tail-recursive and recursive. A simple function returns a value without calling itself, a tail-recursive function calls itself only as the last thing it does before it returns, and a recursive function calls itself and then performs additional computation afterwards.
\\\\
The rules are quite simple:
\begin{itemize}
	\item Constants and calls to other functions have type \textinline{text} providing all the arguments are \textinline{simple}
	\item Recursive calls to the same function have type \textinline{tailrec} providing all the arguments are \textinline{simple}.
	\item An expression that performs computation on its subexpressions (e.g. addition) has type \textinline{simple} if all it's arguments are \textinline{simple}, otherwise it is \textinline{recursive}.
	\item An expression that doesn't perform computation gives the worst type of it's subexpressions.
\end{itemize}
For instance, \camlinline{if simple then simple else tailrec} has type \textinline{tailrec} because it does not perform computation on the result of the \textinline{tailrec}, but \camlinline{if tailrec then simple else tailrec} has type \textinline{recursive} because it uses the result of the condition to determine which branch to choose.
\\\\
Once we have discovered a tail-recursive function, we then convert it to use a while loop as follows. Figure \ref{fig:tailrec} shows this in action:
\begin{enumerate}
	\item We create references for each argument, and for the result
	\item Inside the body of the while loop, we start by loading the arguments from the references
	\item We replace each tail call with loading the correct arguments into the references, and starting the loop body again (`continuing')
	\item If we manage to compute the result, we store it inside the result reference
	\item Once outside the loop, we simply dereference the result reference
\end{enumerate}
This gives us our iterative function body. It can also introduce `broken, unreachable' code, for instance the \camlinline{continue} statement is of type unit so it must return a unit type which is then assigned to result which is not of type unit. This code is clearly broken, but it is also unreachable as the \camlinline{continue} statement restarts the loop instead, so unreachable code analysis is required to eliminate it later. Equally broken is the creation of the result reference if our function does not return an integer, however this can be eliminated via dead-code elimination.

\begin{figure}[h]
\begin{minipage}{0.5\linewidth}
	\begin{minted}[linenos]{OCaml}
let rec fact n acc =
  if n = 0 then
    acc
  else
    fact (n - 1) (n * acc)
	\end{minted}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{minted}[linenos]{OCaml}
let fact n_in acc_in =
  let n_ref = ref n_in in
  let acc_ref = ref acc_in in
  let result_ref = ref 0 in
  while true do
    let n = !n_ref in
    let acc = !acc_ref in
    let result =
      if n = 0 then
        acc
      else
        (n_ref := n - 1;
        acc_ref := n * acc;
        continue)
    in
    result_ref := result
  done
  !result_ref
\end{minted}
\end{minipage}
	\caption{Before and after of tail-recursion optimisation}
	\label{fig:tailrec}
\end{figure}





\section{Intermediate Translation}
%\note{
%	What are the interesting cases?
%	\begin{itemize}
%		\item Mutual recursion: Create closures and then fill them up
%		\item Patterns: Generate code to both check them and destructure
%		\item Match Statements: Go through each block, exit the block early if match fails, until we finish a block and then can exit the whole match
%		\item Boxing of floats
%	\end{itemize}
%}

The original intermediate representation of the compiler was designed to target WebAssembly as simply as possible, and as such was structured and stack-based, with instructions pushing and popping values to the stack, and even containing lists of sub-instructions to evaluate arguments. While this approach allowed me to reach my success criteria quickly, it would have been very difficult to optimise for due to the need to keep track of what data is on the stack while doing analyses.
\\\\
Thus I introduced a new `variable-based' unstructured intermediate representation, where each instruction can take multiple variables as arguments and write a result to one variable, and structure is represented by special begin and end instructions for each structure.
\\\\
The full IR is quite large, but its instructions can be broken down into one of four categories:
\begin{itemize}
	\item Basic operations on variables, e.g. assigning a constant to a variable, copying a variable, and unary and binary operations
	\item Control instructions: those that mark the start and end of blocks, loops and if-else statements, and instructions for jumping out of these structures, as well as the special `fail' instruction.
	\item Memory operations, e.g. creating and loading from tuples/constructs/boxes and closures
	\item A closure-calling instruction, and a direct-call instruction.
\end{itemize}
The types used in the Intermediate Representation are shown in \ref{fig:ir}, with the full IR available in Appendix B.
\begin{figure}[h]
	\begin{minted}[linenos]{OCaml}
type itype =
(* Poly is the supertype of all types represented as i32 in WebAssembly *)
| It_poly | It_bool | It_int | It_pointer | It_unit
| It_float
| It_none (* No type, used for functions with no return type *)
	\end{minted}
	\caption{An overview of the types used in the Intermediate Representation}
	\label{fig:ir}
\end{figure}
Translation into the IR involves translating each function's expression into a list of these intermediate instructions. The top-level AST is translated into it's own sequence of instructions and packaged into a new special `init' function which must take no arguments and return no results, hence the inclusion of the \camlinline{It_none} type. Throughout the translation, we keep track of variables introduced (both temporary and named) and their types. Named variables in the top-level AST become global variables, while temporary variables introduced here become local variables of the init function.
\\\\
This transformation is handled in \textinline{intermediate.ml}, with two main functions:
\begin{itemize}
	\item \camlinline{transform_expr} Recursively transforms an expression into a list of instructions, and a variable for the result of the expression.
	\item \camlinline{transform_pat} This takes a pattern and a variable, and generates a list of instructions that will both test the structure of the value is stored in the variable against the pattern, and destructure this value into new variables. For instance in \camlinline{let (3, a) = x} we will generate code to test the first element of x's tuple to ensure that it is 3, and assign the second element to the variable a.
\end{itemize}
The majority of the transformation is relatively straightforward, however a few cases required additional thought:

\subsection{Closures and (Mutually) Recursive Functions}
Recursive functions require their own closure to call themselves recursively, and the problem is made worse by mutually recursive functions that all require access to each others' closures. Mutually recursive definitions can occur inside expressions, and hence it is not always the case that these closures would be available as global values. This left two possible options:
\begin{enumerate}
	\item Create a new closure for the recursive function or its mutually recursive friends whenever a recursive call occurs, by using the closure variables passed in as arguments to the function.
	\item Include recursive closures inside their own `closure variables'.
\end{enumerate}
I chose the second option because the first option would have been both less efficient, and more difficult to implement. In order to include closures inside their own `closure variables', there are two instructions to create closures. The first one, \textinline{Iins_newclosure}, creates a `new' empty closure for the specified function, while \textinline{Iins_fillclosure} takes an existing empty closure, and fills it up using the variables provided, one of which can be the variable storing the currently empty closure or currently empty closures of mutually recursive friends.

\subsection{Match Statements}
Match statements are handled through nested blocks. The outer block represents the entire match statement, while inner blocks represent individual cases of the match statement. In addition, a temporary variable is introduced to store the result of the entire match statement. The code for an individual case is as follows
\begin{enumerate}
	\item Code for the case's pattern, modified to replace fail instructions (indicating the pattern was not matched), with instructions to jump out of the case block and thus proceed with the next case.
	\item Code for the case's expression
	\item An instruction to copy the expression's result into the match statement's result
	\item An instruction to jump out of the outer match statement block, and thus skip over the remaining cases
\end{enumerate}
In addition, a fail instruction is included after all the cases inside the match block to ensure that if no cases match, execution terminates with a failure.

\subsection{Polymorphism}
Polymorphism is implemented through the special intermediate type \camlinline{It_poly}, which indicates that a variable holds a polymorphic value. This type is used for the arguments and results of all closures, as when an arbitrary closure is called we cannot be sure whether it is a polymorphic function or not and thus must assume it is. It is also used for the contents of tuples, which can also be used polymorphically. 
\\\\
Types that are represented as 32-bit integers in WebAssembly, such as integers, units, booleans and pointers, are all subtypes of this polymorphic type, and hence values of these types can be directly stored in these variables and passed as arguments to functions. Floating point numbers however are not a subtype, as the assignment of a floating point value to an integer variable in WebAssembly is not allowed. Hence we have to `box' these floating-point values, meaning that we allocate a space in memory and store the floating-point value there, passing the pointer to this location instead.
\\\\
The result of the intermediate translation is a list of function objects, which contain the name, intermediate code, and list of variables used by that function. In addition, a list of global variables is produced.





\section{Optimisations on the IR}
%\note{
%	\begin{itemize}
%		\item Unreachable code elimination (needed due to tail-call optimiser's generation of broken unreachable code e.g. assign unit to float because the `break' statement / tail-call gives a unit type)
%		\item Copy propagation (if we have y=x, followed by z = y+y we can replace y with x. By far the most complex of the IR optimisations because we need to know both that the most recent definition of y is y=x, and that x has not changed since then)
%		\item Tuple load elimination (If we know a tuple is (x,y,z), then instead of loading it when matching with (a,b,c), we can just do a=x, b=y, z=c directly. Also works on constructs)
%		\item Dead code elimination (removes useless units, and also tuple creation when tuple usage eliminated by tuple load elimination)
%		\item Ref elimination (eliminates refs that are used as mutable variables only, by using mutable variables)
%	\end{itemize}
%}

Before code generation, a second round of optimisations is performed. These optimisations are predominantly data-flow analyses. Data-flow analysis concerns the movement of data through program code, asking questions like `Will the value assigned here be used?' (Live Variable Analysis) and `Where could the current value of this variable have been assigned?' (Reaching Definitions). Unlike the first round of optimisations, which improved execution time at the expense of code size, these optimisations both improve execution time and decrease code size.
\\\\
Before data-flow analysis can be performed, function code must first be deconstructed into a basic-block graph. A basic block is a sequence of instructions that will always begin execution at the first instruction, and finish at the last, with no jumps out in-between. A conditional jump at the end of one basic block might mean that there are several possible blocks that could be executed next, and hence a graph is required to keep track of these.
\\\\
My approach to construct this graph is to first build a jump-table which records which lines have jumps and where they might jump to. From this we can extract all locations with incoming or outgoing jumps, and use these to split the code into these blocks. We can then use the table to determine the outgoing edges of the basic blocks, and from those record in each basic block its possible predecessor or successor basic blocks.

\subsection{Data-Flow Analysis}
My compiler then implements three forms of data-flow analysis for use in optimisations:
\begin{itemize}
	\item Live Variable Analysis: For each instruction $i$, the variable $x$ is syntactically live at that instruction if there exists an instruction $j$ that is reachable from $i$ without passing any assignments to $x$ and uses the value of $x$. There is another definition of liveness, semantic liveness, whereby a variable is life if it's value will affect the input/output behaviour of the program at some point in the future. As this is generally uncomputable, syntactic liveness is used as a safe approximation.
	\item Reaching Definitions: For each instruction $i$, a set for each variable $x$ of instructions $j$ that assign to $x$ and for which there is a path from $j$ to $i$ without passing another assignment of $x$.
	\item `Available Assignments': For each instruction $i$, a set for each variable $x$ of variables $y$ that, if assigned in an instruction that uses the value of $x$, the value of $x$ has not changed since that assignment to $y$ (but $y$ itself may have been reassigned). This analysis is similar to the more conventional Available Expressions analysis, except that instead of storing expressions for which their operands have not changed since they were computed, it stores the variables assigned from these expressions.
\end{itemize}
An example of these analyses is shown in figure \ref{fig:dataflow}. Line 0 is used to refer to the beginning of the function. Note that on line 8, $y$ is still in the available assignment set for $x$. This is correct, as in all cases where $x$ is assigned to $y$ (as in line 3), $x$ is not modified after this assignment.
\begin{figure}[h]
	\begin{minipage}{0.3\linewidth}
	\begin{minted}[linenos, firstnumber=0]{python}
myfunc(x):
  y = 9
  if cond:
    y = x
    y = 10
  else:
    x = 2
    y = 4
  return y
	\end{minted}
	\end{minipage}
	\begin{minipage}{0.7\linewidth}
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				Line & LVA (at end) & RDs (at start) & AAs (at start) \\
				\hline
				1 & $x$ & $(x, \{0\})$ & \\
				\hline
				3 & & $(x, \{0\}), (y, \{1\})$ & \\
				4 & $y$ & $(x, \{0\}), (y, \{3\})$ & $(x, \{y\})$ \\
				\hline
				6 & & $(x, \{0\}), (y, \{1\})$ & \\
				7 & $y$ & $(x, \{6\}), (y, \{1\})$ & \\
				\hline
				8 & & $(x, \{0, 6\}), (y, \{4, 7\})$ & $(x, \{y\})$ \\
				\hline
			\end{tabular}
		\end{center}
	\end{minipage}
	\caption{An example of the three data-flow analyses}
	\label{fig:dataflow}
\end{figure}
\\\\
For each of these analyses, it is possible to define data-flow equations where we compute these sets for a line by taking either the union or intersection of the sets from all lines that are direct predecessors or successors of the line. The equations for reaching definitions analysis are as follows:
\begin{align*}
\text{in-defs}(i) \quad=&\quad \bigcup_{p\ \in\ \text{pred}(i)} \text{out-defs}(p) \\
\text{out-defs}(i) \quad=&\quad (\text{in-defs}(i) \setminus \text{undef}(i)) \cup \text{def}(i)
\end{align*}
where $\text{undef}(i)$ and $\text{def}(i)$ depend on the instruction $i$. If $i$ does not assign a variable, they are both empty, otherwise if $i$ assigns to $x$, then $\text{undef}(i)$ is all previous assignments to $x$, while $\text{def}(i)$ is the new assignment to $x$, or $(x, \{i\})$. Similar equations can be given for the other analyses, using different def and undef functions: available assignments replaces the big-union with a big-intersection, and live-variables operates backwards using a union of the successor lines to compute the out-lva, and then computing in-lva from that.
\\\\
The algorithm for implementing these data-flow analysis follows from these equations: Put the set for each line to null initially, and iterate over all basic blocks applying these equations (and excluding nulls from intersections, an all null intersection gives the empty set) until there is an iteration in which nothing changes, and then return the result.

\subsection{Transformations From These Analyses}
These analyses allow me to implement a number of transformations:
\begin{itemize}
	\item Unreachable Code Elimination. Unreachable code is code that will never be executed, and hence it is safe to remove. A safe approximation of unreachable code is given by basic blocks that have no predecessors (and are not the first block of the function), and I eliminate the code in these blocks, taking care to retain structural instructions such as those that signify the end of an if-statement or `block'.
	\item Dead Code Elimination. Dead code is code that may be executed, but produces a result that will never be used. Live Variable Analysis can be used, as if a variable is not live at the end of a line where it is assigned, that assignment is never used, and thus if that line contains no side-effects it can be eliminated.
	\item Copy Propagation. Using the results of Reaching Definitions, it becomes possible to check when we see a use of a variable $y$, if the most recent definition of $y$ is $y = x$, and using Available Assignments, we can check that $x$'s value hasn't changed since that assignment. In this case, it is safe to replace a usage of $y$ with $x$. If all usages of $y$ are eliminated, the assignment $y = x$ can then be eliminated with Dead Code Elimination.
	\item Tuple-load Elimination. Similar to copy-propagation, if we encounter the loading of a variable from a tuple $t$, e.g. loading of the 2nd element of a 3 element tuple, we can use Reaching Definitions to see if there is a unique assignment of $t$. If there is and it is of the form $t = (x_1, ..., x_n)$, we can then check using Available Assignments to make sure the $x_i$ we want has not been modified since, and if so we replace the load with $x_i$. If this eliminates all usages of $t$, Dead Code Elimination can then eliminate the construction of the tuple, which can result in a significant speedup. This optimisation is particularly useful when dealing with match statements over multiple variables, as it eliminates the creation of the tuple in that case.
\end{itemize}

\subsection{Other Transformations}
In addition, I perform a transformation I call as `ref elimination'. If a reference is created inside a function, and then only used when it is updated or dereferenced, without being passed as an argument to another function, stored in memory or returned from the function, then the reference can be eliminated and instead a mutable variable is used, which eliminates the memory operations that go with using the reference, and results in a small but noticeable speedup.

\subsection{Ordering of Transformations}
Individually, each transformation is looped until it does not result in a change to the code. However, the order in which the transformations are performed can have a great effect on their effectiveness, for instance if Dead Code Analysis were run only before Tuple-load Elimination, we would not be able to eliminate the unused tuple assignments resulting from Tuple-Load Elimination.
\\\\
Therefore, I order the transformations by running each in order, but then running transformations that might benefit from an earlier transformation again after the earlier transformation. This results in some transformations being run many times, however I do not consider that a problem as the compiler is fast to execute for all samples.

\section{Code Generation}
\note{The stack code generator removes redundant variable saving/loading. Also representation of closures and tuples/constructs/refs}





\section{Summary}
\note{Summarise all that is done with a sentence per stage}





\section{Overview of the Files}







% CHAPTER
\clearpage
\chapter{Evaluation}
\note{This is where Assessors will be looking for signs of success and for evidence of thorough and systematic evaluation as discussed in Section 8.3. Sample output, tables of timings and photographs of workstation screens, oscilloscope traces or circuit boards may be included. A graph that does not indicate confidence intervals will generally leave a professional scientist with a negative impression.}

\note{As with code, voluminous examples of sample output are usually best left to appendices or omitted altogether.}

\note{There are some obvious questions which this chapter will address. How many of the original goals were achieved? Were they proved to have been achieved? Did the program, hardware, or theory really work?}

\note{Assessors are well aware that large programs will very likely include some residual bugs. It should always be possible to demonstrate that a program works in simple cases and it is instructive to demonstrate how close it is to working in a really ambitious case.}

\section{Success Criteria}
\note{Say that it has been satisfied, and list the success criteria}

\section{End To End Tester}
\note{Could do with a -all mode to test all combinations of disabled / enabled optimisations}

\section{Benchmarks / Benchmark Driven Optimisations}
\note{
	\begin{itemize}
		\item Gcd: main target of optimisations. Went from 122ms to 8ms thanks to all of them.
		\item others: they improve (e.g. especially with direct call generation), but not as much with the other optimisations
		\item Make some performance graphs of the final result vs other execution engines, and of different optimisations enabled/disabled (can do both time and memory for that).
	\end{itemize}
}

% TODO DONT FORGET ALL THE WORK IN MAKING THE CODE SHORTER

% TODO HOW IT WAS EVALUATED
% TODO TEST SYSTEM AND BENCHMARKS




\clearpage
\chapter{Conclusion}
\note{This chapter is likely to be very short and it may well refer back to the Introduction. It might properly explain how you would have planned the project if starting again with the benefit of hindsight. }

\note{
	\begin{itemize}
		\item SSA based IR?
		\item More optimisations e.g. reverse copy-propagation, mutually recursive tail call optimisation, better match statements, better code generator that can re-order non side effecting instructions, speedy append implementation
		\item More feature support: strings, records and mutable records, modules, named/optional function arguments, using the operators as function arguments (e.g. List.reduce ~f:(+) nums to sum a list)
	\end{itemize}
}

% TODO CONCLUDE THE DOCUMENT
% TODO IVE SUCCESSFULLY MET SUCCESS CRITERIA, SUMMARY OF EVERYTHING
% TODO WHAT LESSONS DID I LEARN
% TODO WHAT WORK COULD YOU DO IN THE FUTURE




\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography[title={Bibliography}]
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix
% Assessors like to see some sample code or example circuit diagrams, and appendices are the sensible places to include such items. Accordingly, software and hardware projects should incorporate appropriate appendices. Note that the 12,000 word limit does not include material in the appendices, but only in extremely unusual circumstances may appendices exceed 10-15 pages - if you feel that such unusual circumstances might apply to you you should ask your Director of Studies and Supervisor to apply to the Chairman of Examiners. It is quite in order to have no appendices. Appendices should appear between the bibliography and the project proposal. 

\chapter{The First Appendix}

Things in appendix A


\clearpage

\chapter{The Intermediate Representation}

\note{TODO FIX THE COMMENTS, SOME ARE FROM OLD IR}

\begin{minted}{OCaml}
type itype =
| It_poly | It_bool | It_int | It_pointer | It_unit
| It_float
| It_none

type iunop =
| Iun_neg (* Negate *)
| Iun_eqz (* Equals zero *)

type ibinop =
| Ibin_add | Ibin_sub | Ibin_mul | Ibin_div | Ibin_rem (* Arithmetic *)
| Ibin_and | Ibin_or (* Logic *)
| Ibin_eq | Ibin_ne (* Equality *)
| Ibin_lt | Ibin_le | Ibin_gt | Ibin_ge (* Comparison *)

type iscope = Local | Global (* Variable scope *)

type ivariable = iscope * string

type iinstruction =
(* Create a new var from a constant *)
(* type of variable, name of variable *)
| Iins_setvar of itype * ivariable * string
(* Copy a variable into another *)
(* type of variable, name of new variable, name of old variable *)
| Iins_copyvar of itype * ivariable * ivariable
(* Return var *)
(* type of variable, name of variable *)
| Iins_return of itype * ivariable
(* Unary operation using one stack value *)
(* type of operand, unary operation, result var, input var *)
| Iins_unop of itype * iunop * ivariable * ivariable
(* Binary operation using two stack values *)
(* type of operands, binary operation *)
| Iins_binop of itype * ibinop * ivariable * ivariable * ivariable
(* Make a new closure for specified function and tuple type, and put it in given variable *)
(* type of function, name of function, type of closure variables, variable to put closure in *)
| Iins_newclosure of iftype * string * ituptype * ivariable
(* Fill a closure in the named variable using the code to generate those values *)
(* type of closure variables, name of variable, list of variables to copy in *)
| Iins_fillclosure of ituptype * ivariable * ivariable list
(* Call closure in variable using argument generated from code *)
(* type of function, output variable, closure variable, variable for argument *)
| Iins_callclosure of iftype * ivariable * ivariable * ivariable
(* Directly call a function *)
(* output variable, name of function, type of args, arg vars *)
| Iins_calldirect of ivariable * string * ituptype * (ivariable list)
(* Start a block *)
(* name of block *)
| Iins_startblock of string
(* End a block *)
(* name of block *)
| Iins_endblock of string
(* Exit from the named block *)
(* name of block *)
| Iins_exitblock of string
(* Exit from the named block if variable is true *)
(* name of block *)
| Iins_exitblockif of string * ivariable
(* Start an if statement *)
(* name of block, condition var *)
| Iins_startif of string * ivariable
(* Else clause of an if statement *)
(* name of block *)
| Iins_else of string

(* End an if statement *)
(* name of block *)
| Iins_endif of string
(* Starts a loop, loops until an exitblock or exitblockif *)
(* Name of escape block (to break to), name of loop block (to continue to) *)
| Iins_startloop of string * string
(* Ends a loop *)
(* Name of break block, name of continue block *)
| Iins_endloop of string * string
(* Create a tuple from the given vars, push pointer to stack
 * and put it in that variable *)
(* type of tuple, name of variable, code to generate each part of tuple *)
| Iins_pushtuple of ituptype * ivariable * ivariable list
(* Pop tuple, push its value at index i to the stack *)
(* type of tuple, index in tuple, output var, tuple var *)
| Iins_loadtupleindex of ituptype * int * ivariable * ivariable
(* Create a construct from the given vars, push pointer to stack and put it in variable *)
(* type of construct arguments, name of variable, id of construct, arguments *)
| Iins_pushconstruct of ituptype * ivariable * int * ivariable list
(* Pop construct, push its value at index i to the stack *)
(* type of construct arguments, index in arguments, output variable, tuple variable *)
| Iins_loadconstructindex of ituptype * int * ivariable * ivariable
(* Pop construct, push its id to the stack *)
(* output variable, construct variable *)
| Iins_loadconstructid of ivariable * ivariable
(* Box a value (on stack) of a type that needs boxing, or for a ref *)
(* unboxed type, unboxped variable, variable to store boxed result *)
| Iins_newbox of itype * ivariable * ivariable
(* Update a boxed value, useful for refs *)
(* unboxed type, unboxped variable, boxed variable *)
| Iins_updatebox of itype * ivariable * ivariable
(* Unbox a value / dereference a ref *)
(* unboxped type, wrapped variable, unboxped target variable *)
| Iins_unbox of itype * ivariable * ivariable
(* Fail *)
(* No parameters *)
| Iins_fail
\end{minted}


\clearpage

\chapter{Project Proposal}
\clearpage

\input{propbody}

\end{document}
